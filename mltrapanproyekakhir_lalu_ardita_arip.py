# -*- coding: utf-8 -*-
"""MLTrapanProyekAkhir_Lalu Ardita Arip.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HHkMRUDSxaLOteX257hn5M8rtiLgVIH1

#Proyek Pertama
#Nama : Lalu Ardita Arip
#SIB ID : M302X0788
#Email : ardita.arip18@gmail.com
#Email :M302X0788@dicoding.org
#alamat : Lombok Timur, NTB
#kode kelas : M-08

link dataset :https://www.kaggle.com/code/nezarabdilahprakasa/big-data-analysis-using-pysprak-movie-recommed/data?select=tags.csv

melakukan loading data agar data lebih mudah diproses
"""

import pandas as pd
import numpy as np 
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

links = pd.read_csv('links.csv')
movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
tags = pd.read_csv('tags.csv')

print('Jumlah data link  : ', len(links.movieId.unique()))
print('Jumlah data movie : ', len(movies.movieId.unique()))
print('Jumlah data ratings dari user : ', len(ratings.userId.unique()))
print('Jumlah data ratings dari movie : ', len(ratings.movieId.unique()))
print('Jumlah data tags : ', len(tags.movieId.unique()))

"""membaca data dengan fungsi pandas.read_csv """

links.info()

"""melakukan eksplorasi terhadap variabel links yang merupakan daftar link movie."""

movies.info()

"""melakukan eksplorasi terhadap variabel movie yang merupakan daftar link movie."""

ratings.info()

"""melakukan eksplorasi terhadap variabel rating yang merupakan daftar link movie."""

tags.info()

"""melakukan eksplorasi terhadap variabel tags yang merupakan daftar link movie."""

ratings.head()

"""mengecek nilai dari data rating"""

ratings.describe()

"""diketahui bahwa nilai max adalah 5 dan min 0.5 sehingga dapat dibuat sekal anatar 0.5 sampai 5"""

film = np.concatenate((
    links.movieId.unique(),
    movies.movieId.unique(),
    ratings.movieId.unique(),
    tags.movieId.unique(),
))
film = np.sort(np.unique(film)) 
print('Jumlah data seluruh film berdasarkan movieID: ', len(film))

"""menggabungkan file-file dengan menggunakan fungsi concatenate dari movieId menjadi satu dalam variabel film"""

user = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique(),
))
user = np.sort(np.unique(user)) 
print('Jumlah data seluruh user: ', len(user))

"""menggabungkan file-file dengan menggunakan fungsi concatenate dari userId menjadi satu dalam variabel user"""

info_film = pd.concat([links, movies, ratings, tags])
film = pd.merge(ratings, info_film , on='movieId', how='left')
film

"""file links, movies, ratings dan tags digabungkan kedalam dataframe info_film, serta menggabungkan dataframe rating dan info_film."""

film.isnull().sum()

"""mengecek missing value"""

film.groupby('movieId').sum()

"""menggabungkan beberapa rating berdasakan movieId"""

film_rate = ratings
film_rate

"""membuat variabel film_rate dengan definisi ratings"""

film_name = pd.merge(film_rate, movies[['movieId','title','genres']], on='movieId', how='left')
film_name

"""menggabungkan film_rate dengan variabel ratings"""

Sfilm = pd.merge(film_name, tags[['movieId','tag']], on='movieId', how='left')
Sfilm

"""dataframe tags dan film_name digabungkan kedalam variabel Sfilm berdasakan kesamaan movieId"""

Sfilm.isnull().sum()

"""mengecek missing valeu"""

Sfilm_clean = Sfilm.dropna()
Sfilm_clean

"""membersihkan missing value"""

Sfilm_clean.isnull().sum()

"""mengecek missing value yang telah dibersihkan"""

filmFix = Sfilm_clean.sort_values('movieId', ascending=True)
filmFix

"""membuat varibael filmFix yang berisi data film yang sudah diurutkan"""

len(filmFix.movieId.unique())

"""dari hasil di atas di peroleh bahwa jumlah data yang bisa di gunakan sebagai sample sebanyak 630"""

pembaharuanFilm = filmFix.drop_duplicates('movieId')
pembaharuanFilm

"""membuat variabel pembaharuanFilm yang berisi data uniq dan lakukan drop pada data filmFix"""

id = pembaharuanFilm['movieId'].tolist()
title = pembaharuanFilm['title'].tolist()
genre = pembaharuanFilm['genres'].tolist()
 
print('jumlah ID   :',len(id))
print('jumlah judul:',len(title))
print('jumlah genre:',len(genre))

"""mengkonversi data mengguankan fungsi tolist()"""

filmBaru = pd.DataFrame({
    'id': id,
    'title': title,
    'genre': genre
})
filmBaru

"""membuat dictionary untuk menentukan key dan value"""

tf = TfidfVectorizer()
tf.fit(filmBaru['genre']) 
tf.get_feature_names()

"""melakukan development terhadap model dengan content based filtering menggunakan fungsi TFIDFVectorizer()"""

tfidf_matrix = tf.fit_transform(filmBaru['genre']) 
tfidf_matrix.shape

"""mengubah kedalam bentuk matrik"""

tfidf_matrix.todense()

"""menggunakan bentuk todense() untuk melihat bentuk matrik"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=filmBaru.title
).sample(20, axis=1).sample(10, axis=0)

"""melihat bentuk matrik"""

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""melakukan perhitungan terhadap derjat kesamaan antara movie dangan teknik cosine similarity."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=filmBaru['title'], columns=filmBaru['title'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""melihat kesamaan bentuk matrik dari setiap film"""

def film_recommendations(title, similarity_data=cosine_sim_df, items=filmBaru[['title', 'genre']], k=10):
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""membuat fungsi untuk membuat rekomendasi."""

filmBaru[filmBaru.title.eq('Bring It On (2000)')]

"""menemukan rekomendasi berdasarkan 'Bring It On (2000)'"""

film_recommendations('Bring It On (2000)')

"""rekomendasi berdasarkan 'Bring It On (2000)'"""

df = ratings
df

"""memasukan variabel rating kedalam variabel df"""

user_ids = df['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('list userID: ', user_ids)
print('encoded userID : ', user_to_user_encoded)
print('encoded angka ke userID: ', user_encoded_to_user)

"""melakukan procesing"""

film_ids = df['movieId'].unique().tolist()
film_to_film_encoded = {x: i for i, x in enumerate(film_ids)}
film_encoded_to_film = {i: x for i, x in enumerate(film_ids)}
df['genres'] = df['userId'].map(user_to_user_encoded)
df['movies'] = df['movieId'].map(film_to_film_encoded)

"""melakukan procesing berdasarkan movieId"""

num_users = len(user_to_user_encoded) 
num_film = len(film_encoded_to_film)
df['ratings'] = df['rating'].values.astype(np.float32)
min_rating = min(df['rating'])
max_rating = max(df['rating'])
print(num_users)
print(num_film)
print('Number of User: {}, Number of film: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_film, min_rating, max_rating
))

"""melkukan pengecekan terhadap User: 671, Number of film: 9066, Min Rating: 0.5, Max Rating: 5.0 dan di proleh hasil demikian."""

df = df.sample(frac=1, random_state=42)
df

"""melakukan pengecekkan terhadap data set"""

x = df[['genres', 'movies']].values
y = df['ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""membagi dataset menjadi data train dan valid menjadi 80:20"""

import tensorflow as tf
from tensorflow import keras
class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_film, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_film = num_film
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.film_embedding = layers.Embedding( # layer embeddings movies
        num_film,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.film_bias = layers.Embedding(num_film, 1) # layer embedding movies bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    film_vector = self.film_embedding(inputs[:, 1]) # memanggil layer embedding 3
    film_bias = self.film_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_film = tf.tensordot(user_vector, film_vector, 2) 
 
    x = dot_user_film + user_bias + film_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""melakukan proses tarining """

model = RecommenderNet(num_users, num_film, 50) # inisialisasi model
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""melakukan evaluasi emngguankan RMSE"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""melakukan proses training dengan epoch sebesar 50 """

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""melakukan visualisasi pada hasil proses"""

film_df = filmBaru
df = pd.read_csv('ratings.csv')
user_id = df.userId.sample(1).iloc[0]
film_watched_by_user = df[df.userId == user_id]
film_not_watched = film_df[~film_df['id'].isin(film_watched_by_user.movieId.values)]['id'] 
film_not_watched = list(
    set(film_not_watched)
    .intersection(set(film_to_film_encoded.keys()))
)
film_not_watched = [[film_to_film_encoded.get(x)] for x in film_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_film_array = np.hstack(
    ([[user_encoder]] * len(film_not_watched), film_not_watched)
)

"""mendapatakn rekomendasi move berdasarkan rating """

ratings = model.predict(user_film_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_film_ids = [
    film_encoded_to_film.get(film_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)
 
top_film_user = (
    film_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
film_df_rows = film_df[film_df['id'].isin(top_film_user)]
for row in film_df_rows.itertuples():
    print(row.title, ':', row.genre)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_film = film_df[film_df['id'].isin(recommended_film_ids)]
for row in recommended_film.itertuples():
    print(row.title, ':', row.genre)

"""menggunakan fungsi model.predict() dan di proleh hasil berupa 10 rekomendasi filem berdasarkan reting """